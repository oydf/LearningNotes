# 1、分而治之/hash映射 + hash_map统计 + 堆/快排/归排
    *海量日志数据，提取出某日访问百度次数最多的那个IP。
    分析：
        海量数据即无法一次装入内存进行处理：大文件化小文件，小文件逐个处理；ip的总量为2^32次方，总量有限，hash映射法。
    解决方案：
        将海量日志数据中的ip提取存入一个文件中，对该文件的内容进行hash映射，分别映射到1000个小文件中，小文件用hash_map对ip频率进行统计，最后用堆对每个小文件中的最高频ip进行统计，得出最高频的ip

    *寻找热门查询，300万个查询字符串中统计最热门的10个查询
    分析：
        数据规模较小，考虑将数据一次性装入内存进行处理。top K问题
    解决方案：
        维护一个key为字符串，value为出现次数的hashtable，对数据进行统计；然后维护一个10个元素的小顶堆，堆满后每次新元素与堆顶元素比较，大于则更新堆，遍历比较完成后，堆元素即为top K。
        trie字典树统计，堆选top K.
    
    *有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
        分析：
            文件不大，但是内存限制，先划分大文件为小文件。
        解决方案：
            顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，继续hash；
            对每个小文件统计频率；最小堆选出top 100。

    *海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
        分析：
            如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP 10的数据元素。堆排序：在每台电脑上求出TOP10，100台的top 10统计top 10.
            如果同一个元素重复出现在不同的电脑中:
                遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法;
                直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP 10。
    
    *有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
        分析：（将完全没有规律的数据，遍历依次重新映射）
            hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
            hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。
            堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）

            直接用trie遍历所有数据计数，再排序

    *给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url
        分析：
            遍历a，hash（url）%1000映射到1000个文件，遍历b，hash（url）%1000映射到1000个文件，对应相同的URL必定会在对应的小文件中；对应小文件用hash_set，将a的url放入set，用b文件的url去对比相同的url。
    
    *怎么在海量数据中找出重复次数最多的一个
        分析：
            先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求。

    *上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。
        分析：
            hash_map计数，取top N。

    *一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
        分析：
            trie字典树先统计每个词的频度，最小堆求top 10。O(logle)+O(n*log10)

    *1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现
        分析：
            用hash_set遍历字符串insert元素，最后set就是不重复的字符串集合。
            trie建立字典，然后遍历insert进入set中。

    *一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解
        分析：
            首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。

# 2、多层划分
    多层划分----其实本质上还是分而治之的思想，重在“分”的技巧上！
　　适用范围：第k大，中位数，不重复或重复的数字
　　基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，最后在一个可以接受的范围内进行处理。

    *2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。
        分析：
            使用2_bit_map，一共有2^32个整数，用两个bit表示对应整数的状态，00未出现，01出现一次，11出现大于一次，扫描一遍2.5亿个整数，更新对应位图的值，最后选出位图00对应的整数。
    
    *5亿个int找它们的中位数
        分析：
            整数的总数为2^32，用一个2^32容量的数组来做桶，每次整数值与数组索引值相等，则索引对应的数值加1，统计完5亿个数后，桶数组从头累加索引值的次数，当累加值=2.5亿时的索引即为解。
    
# 3、Bloom filter/Bitmap
    可以用来实现数据字典，进行数据的判重，或者集合求交集
    
    bit_map示例：
        给一台普通PC，2G内存，要求处理一个包含40亿个不重复并且没有排过序的无符号的int整数，给出一个整数，问如果快速地判断这个整数是否在文件40亿个数据当中？
        分析：
            40 0000 0000 * 4 = 160 0000 0000 /1024/1024/1024 = 14.9GB，这个数量明显不能一次装入内存进行处理。
            用一个bit位来标识一个整数（4*8=32位）40 0000 0000 * 4 / 8 /1024 /1024 = 476M，可以装入内存处理，那么需要int tmp[1+N/32]的连续数组空间，那么：
                tmp[0]:0~31 tmp[1]:32~63 tmp[2]:64~95 。。。。
            N/32将N映射到对应的tmp下标，N%32,将N映射到位图的第几位。每次访问数字过后，相应的位置的位置1。
            空间复杂度随集合内最大元素增大而线性增大。

    布隆过滤器：
        布隆过滤器引入了k(k>1) 个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。
        遍历元素，将每个元素通过K个hash函数映射bitmap的K个位置上置一，判断新元素是否出现过，观察其K个对应的bit位上是否都置1（很可能在集合中）。有0则一定不在集合中。
        随着集合中的元素不断输入过滤器中(n nn增大)，误差将越来越大。但是，当Bitmap的大小m（指bit数）足够大时，比如比所有可能出现的不重复元素个数还要大10倍以上时，错误概率是可以接受的。

# 4、Trie树/数据库/倒排索引
    trie树：
        数据量大，重复多，但是数据种类小可以放入内存。
    数据库索引：
        大数据量的增删改查，利用数据的设计实现方法，对海量数据的增删改查进行处理。
    倒排索引：
        搜索引擎，关键字查询。用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。

# 5、外排序
    大数据的排序，去重，外排序的归并方法，置换选择败者树原理，最优归并树。
    1、采用适当的内部排序方法对输入文件的每个片段进行排序，将排好序的片段（成为归并段）写到外部存储器中（通常由一个可用的磁盘作为临时缓冲区），这样临时缓冲区中的每个归并段的内容是有序的。
    2、利用归并算法，归并第一阶段生成的归并段，直到只剩下一个归并段为止。

# 6、分布式处理之Mapreduce
     MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。将数据交给不同的机器去处理，数据划分，结果归约。

     *一致性hash：
        在分布式负载均衡方案中，可以使用hash算法让一部分固定请求映射到同一机器上处理，但是普通的hash算法伸缩性很差，当上线新机器或者下线旧机器时，会造成映射关系大量失效，索引在hash算法基础上利用hash环进行改进。普通hash是对服务器数量进行取模映射，一致性hash是对整个数值空间2^32进行取模的，将2^32个点连成一个环，称作hash环。

        hash过程：
            hash（A服务器ip）% 2^32 = A,对应于hash环上一个点nodeA;
            hash（B服务器ip）% 2^32 = B,对应于hash环上一个点nodeB;
            hash（C服务器ip）% 2^32 = C,对应于hash环上一个点nodeC;
            将三台服务器映射到hash环上，现在来了客户请求R1，R2,R3,R4,R5
            hash（R1） % 2^32 = r1,对应于hash环上一个点node1；
            hash（R2） % 2^32 = r2,对应于hash环上一个点node2；
            hash（R3） % 2^32 = r3,对应于hash环上一个点node3；
            hash（R4） % 2^32 = r4,对应于hash环上一个点node4；
            hash（R5） % 2^32 = r1,对应于hash环上一个点node5；
            将五个客户请求映射hash环上，每个请求会被分配到环顺时针方向最近的服务器上，当有服务器下线或者上线时，部分请求访问服务器不变，部分请求会被重新分配到当前顺时针距离自己最近的服务器上，避免了失效问题。
            但是当服务器数量很少的情况下，服务器都映射到环上的一端，就会导致hash环的倾斜，进一步导致大量请求分配到一台服务器，负载不均衡，为解决此问题，引入了虚拟节点。将物理服务器虚拟出多个虚拟节点，映射到hash环上，节点数量多了过后，节点分别也就慢慢趋于均衡了。然后虚拟节点都对应了物理节点，请求先找到虚拟节点，在找到物理节点，是每个物理节点尽可能处理均衡的请求。



    



    






            


        

